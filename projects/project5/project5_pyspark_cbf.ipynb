{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49baf479-dc1a-44a1-88ad-b0a1ee4c6a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=>                                                       (1 + 8) / 35]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, udf, collect_list, expr, avg\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.ml.evaluation   import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import expr, col, arrays_zip, sum as _sum, count as _count\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbb5c3-de06-4ee0-88aa-0339665f0ef8",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897ba141-d1f7-4beb-aabd-1915b8f76ab4",
   "metadata": {},
   "source": [
    "Below, I attempt to recreate my project 4 TF-IDF content-based recommendation system on Goodreads data. This time, rather than limiting myself to one genre (comics / graphic novels), I leverage the distributed computing power of pyspark to bring in all available data across genres.\n",
    "\n",
    "First, I can initiate spark (my computer has 8GB of RAM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbfe3b3d-d5f3-463f-a9e3-81efa4294ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/15 02:00:16 WARN Utils: Your hostname, Jacobs-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.7 instead (on interface en0)\n",
      "25/07/15 02:00:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/15 02:00:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52328)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jacobsilver/miniconda3/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/jacobsilver/miniconda3/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/jacobsilver/miniconda3/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/jacobsilver/miniconda3/lib/python3.12/socketserver.py\", line 766, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/jacobsilver/miniconda3/lib/python3.12/site-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/jacobsilver/miniconda3/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/Users/jacobsilver/miniconda3/lib/python3.12/site-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jacobsilver/miniconda3/lib/python3.12/site-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DistributedTFIDFRecs\") \\\n",
    "    .config(\"spark.driver.memory\",\"8g\") \\\n",
    "    .config(\"spark.executor.memory\",\"8g\") \\\n",
    "    .config(\"spark.sql.ansi.enabled\",\"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebab0a5-8ee7-4c24-9fd8-99b249adb39a",
   "metadata": {},
   "source": [
    "Next, I load my parquet files. These were created from a json and csv file from this data ([https://cseweb.ucsd.edu/~jmcauley/datasets/goodreads.html]) in the notebook file build_parquets.ipynb sharing this folder. The interactions data required a bit of column fixing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15aba2b8-15b6-4ab4-bf7a-888eec836531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your parquet files\n",
    "df_b = spark.read.parquet(\"data/goodreads_books.parquet\")\n",
    "\n",
    "#df_i (issue with column headers)\n",
    "df_i = spark.read.parquet(\"data/goodreads_interactions.parquet\")\n",
    "df_i= df_i.toDF(\n",
    "    \"user_id\",\n",
    "    \"book_id\",\n",
    "    \"is_read\",\n",
    "    \"rating\",\n",
    "    \"is_reviewed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439cd443-1d61-4266-88c9-65bd53e86d77",
   "metadata": {},
   "source": [
    "As with project 4, I'll trim the data for noise management and operability, though it remains far larger than anything I've worked with previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acd907a7-a830-476e-8df2-1402094fc6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify popular items\n",
    "popular = df_i.groupBy(\"book_id\") \\\n",
    "             .count() \\\n",
    "             .filter(\"count >= 50\") \\\n",
    "             .select(\"book_id\")\n",
    "\n",
    "#restrict interactions to those popular items\n",
    "df_i_pop = df_i.join(popular, on=\"book_id\")\n",
    "\n",
    "#down‐sample the interactions\n",
    "df_i2 = df_i_pop.sample(fraction=0.01, seed=905)\n",
    "\n",
    "#pplit that sample into train/test\n",
    "train, test = df_i2.randomSplit([0.8, 0.2], seed=905)\n",
    "\n",
    "#now restrict your book metadata to exactly those sampled books\n",
    "sampled_books = df_i2.select(\"book_id\").distinct()\n",
    "df_b = df_b.join(sampled_books, on=\"book_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a019fc92-a534-4d50-89aa-6adc1a19a2cb",
   "metadata": {},
   "source": [
    "Now I can build a TF-IDF vectorization pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb1ce855-15c9-44ea-9796-3ae37fdec78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"description\", outputCol=\"words\")\n",
    "hashTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2048)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashTF, idf])\n",
    "\n",
    "model = pipeline.fit(df_b)\n",
    "df_b_feat  = model.transform(df_b).select(\"book_id\", \"features\")\n",
    "\n",
    "#join interactions → weighted vectors\n",
    "df_ui = train.join(df_b_feat, on=\"book_id\") \\\n",
    "            .select(\"user_id\",\"book_id\",\"rating\",\"features\")\n",
    "\n",
    "def weighted(v, r):\n",
    "    arr = v.toArray()\n",
    "    return Vectors.dense(arr * float(r))\n",
    "\n",
    "weighted_udf = udf(weighted, VectorUDT())\n",
    "df_w = df_ui.withColumn(\"weighted\", weighted_udf(col(\"features\"), col(\"rating\")))\n",
    "\n",
    "#build user profiles via DataFrame API\n",
    "summ = Summarizer.metrics(\"sum\")\n",
    "agg = (\n",
    "    df_w\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(summ.summary(df_w.weighted).alias(\"stats\"))\n",
    ")\n",
    "user_profiles = (\n",
    "    agg\n",
    "    .select(\"user_id\", col(\"stats\")[\"sum\"].alias(\"user_profile\"))\n",
    "    .persist()\n",
    ")\n",
    "\n",
    "#collect & broadcast as numpy arrays\n",
    "items = df_b_feat.collect()\n",
    "item_map = { r.book_id: r.features.toArray() for r in items }\n",
    "item_bcast = sc.broadcast(item_map)\n",
    "\n",
    "#robust recommendation UDF\n",
    "def recommend_np(profile, k=10):\n",
    "    ups = profile.toArray()\n",
    "    nu  = np.linalg.norm(ups)\n",
    "    if nu == 0:\n",
    "        return []                  # nothing to recommend\n",
    "    sims = []\n",
    "    for bid, arr in item_bcast.value.items():\n",
    "        nv = np.linalg.norm(arr)\n",
    "        if nv == 0:\n",
    "            continue              # skip zero‐vector items\n",
    "        sims.append((bid, float(np.dot(ups, arr)/(nu*nv))))\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [str(bid) for bid,_ in sims[:k]]\n",
    "\n",
    "rec_udf = udf(recommend_np, ArrayType(StringType()))\n",
    "recs = user_profiles.withColumn(\"recommendations\", rec_udf(col(\"user_profile\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40772893-4881-467c-85e5-f4e4132f0835",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164a4dbe-d771-496c-bc21-e3f44e1e3b81",
   "metadata": {},
   "source": [
    "In Project 4, it was not really even possible for me to load more than one genre's worth of data, and it happened to be one of the smallest datasets available. I actually abandoned subject areas I had greater interest in (such as history/biography) due to an inability to process any results. In that sense, once I'm dealing with a few million ratings and 10s of thousands of items/users, I believe it's essentially a necessity to move toward a distributed environment, even if I then intend to filter the data down.\n",
    "\n",
    "That said, I wish that pyspark on a local Jupyter notebook allowed for easier usage of traditional pandas syntax, which offers a great deal of analytical flexibility. In the future, I would try to achieve the best of both worlds by borrowing computational power from the cloud, such as via Microsoft Azure or another online ML platform, while still retaining the ability to use any python package of my choosing. While I could ultimately build a recommendation system in this format, evaluation, and even something as relatively simple as EDA, became a challenging and laborious effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
